{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377093a5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==================== ENHANCED FEATURE EXTRACTION ====================\n",
    "\n",
    "class OptimizedFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        # Increased TF-IDF dimensions for better text capture\n",
    "        self.tfidf_name = TfidfVectorizer(\n",
    "            max_features=220, ngram_range=(1, 3), min_df=2, max_df=0.90,\n",
    "            sublinear_tf=True, strip_accents='unicode', token_pattern=r'\\b\\w+\\b'\n",
    "        )\n",
    "        self.tfidf_bullets = TfidfVectorizer(\n",
    "            max_features=280, ngram_range=(1, 3), min_df=2, max_df=0.90,\n",
    "            sublinear_tf=True, strip_accents='unicode'\n",
    "        )\n",
    "        self.char_vectorizer = TfidfVectorizer(\n",
    "            analyzer='char', ngram_range=(3, 4), max_features=50, min_df=2\n",
    "        )\n",
    "\n",
    "        # Enhanced SVD for better dimensionality reduction\n",
    "        self.svd_name = TruncatedSVD(n_components=42, random_state=42)\n",
    "        self.svd_bullets = TruncatedSVD(n_components=55, random_state=42)\n",
    "        self.svd_char = TruncatedSVD(n_components=18, random_state=42)\n",
    "\n",
    "        self.image_pca = None\n",
    "        self.scaler = RobustScaler()\n",
    "\n",
    "        self.unit_conversions = {\n",
    "            'oz': 1, 'ounce': 1, 'lb': 16, 'pound': 16, 'g': 0.035274, 'gram': 0.035274,\n",
    "            'kg': 35.274, 'ml': 0.033814, 'milliliter': 0.033814, 'l': 33.814, 'liter': 33.814,\n",
    "            'fl oz': 1, 'fluid ounce': 1, 'fl. oz': 1, 'floz': 1\n",
    "        }\n",
    "\n",
    "        self.brand_stats = {}\n",
    "        self.category_stats = {}\n",
    "        self.le_category = LabelEncoder()\n",
    "\n",
    "    def extract_value_field(self, text):\n",
    "        if pd.isna(text):\n",
    "            return None\n",
    "        match = re.search(r'Value:\\s*(\\d+\\.?\\d*)', str(text), re.IGNORECASE)\n",
    "        return float(match.group(1)) if match else None\n",
    "\n",
    "    def extract_unit_field(self, text):\n",
    "        if pd.isna(text):\n",
    "            return 'unknown'\n",
    "        match = re.search(r'Unit:\\s*([^\\n]+)', str(text), re.IGNORECASE)\n",
    "        return match.group(1).strip().lower() if match else 'unknown'\n",
    "\n",
    "    def extract_size_features_advanced(self, text):\n",
    "        if pd.isna(text):\n",
    "            return self._empty_size_features()\n",
    "        text = str(text).lower()\n",
    "        value = self.extract_value_field(text)\n",
    "        unit = self.extract_unit_field(text)\n",
    "\n",
    "        pack_patterns = [\n",
    "            r'pack\\s+of\\s+(\\d+)', r'\\(pack\\s+of\\s+(\\d+)\\)', r'(\\d+)\\s*-?\\s*pack',\n",
    "            r'(\\d+)\\s+count', r'(\\d+)\\s*ct\\b', r'case\\s+of\\s+(\\d+)',\n",
    "            r'(\\d+)\\s+per\\s+case', r'(\\d+)\\s*pk\\b', r'(\\d+)\\s+units?', r'set\\s+of\\s+(\\d+)'\n",
    "        ]\n",
    "        pack_count = 1\n",
    "        for pattern in pack_patterns:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                pack_count = int(match.group(1))\n",
    "                break\n",
    "\n",
    "        size_oz = 0\n",
    "        if value and unit and unit in self.unit_conversions:\n",
    "            size_oz = value * self.unit_conversions[unit]\n",
    "\n",
    "        if size_oz == 0:\n",
    "            size_patterns = [\n",
    "                r'(\\d+\\.?\\d*)\\s*x?\\s*(fl\\s*oz|fluid\\s*ounce|fl\\.\\s*oz)',\n",
    "                r'(\\d+\\.?\\d*)\\s*-?\\s*(oz|ounce)(?!\\s*(pack|ct|count))',\n",
    "                r'(\\d+\\.?\\d*)\\s*(lb|lbs|pound|pounds)',\n",
    "                r'(\\d+\\.?\\d*)\\s*(g|gram|grams)(?!\\s*(pack|ct))',\n",
    "                r'(\\d+\\.?\\d*)\\s*(kg|kilogram)', r'(\\d+\\.?\\d*)\\s*(ml|milliliter)',\n",
    "                r'(\\d+\\.?\\d*)\\s*(l|liter)(?!bs)',\n",
    "            ]\n",
    "            for pattern in size_patterns:\n",
    "                match = re.search(pattern, text)\n",
    "                if match:\n",
    "                    size = float(match.group(1))\n",
    "                    unit_type = match.group(2).strip()\n",
    "                    size_oz = size * self.unit_conversions.get(unit_type, 1)\n",
    "                    break\n",
    "\n",
    "        total_volume = size_oz * pack_count\n",
    "        size_oz = min(size_oz, 500)\n",
    "        total_volume = min(total_volume, 2000)\n",
    "        pack_count = min(pack_count, 100)\n",
    "\n",
    "        return {\n",
    "            'size_oz': size_oz,\n",
    "            'pack_count': pack_count,\n",
    "            'total_volume_oz': total_volume,\n",
    "            'has_size_info': 1 if size_oz > 0 else 0,\n",
    "            'log_size': np.log1p(size_oz),\n",
    "            'log_total_volume': np.log1p(total_volume),\n",
    "            'log_pack': np.log1p(pack_count),\n",
    "            'sqrt_size': np.sqrt(size_oz) if size_oz > 0 else 0,\n",
    "            'is_bulk': 1 if pack_count >= 6 else 0,\n",
    "            'is_single': 1 if pack_count == 1 else 0,\n",
    "            'value_field': value if value else 0,\n",
    "            'has_value_field': 1 if value else 0,\n",
    "        }\n",
    "\n",
    "    def _empty_size_features(self):\n",
    "        return {\n",
    "            'size_oz': 0, 'pack_count': 1, 'total_volume_oz': 0, 'has_size_info': 0,\n",
    "            'log_size': 0, 'log_total_volume': 0, 'log_pack': 0, 'sqrt_size': 0,\n",
    "            'is_bulk': 0, 'is_single': 1, 'value_field': 0, 'has_value_field': 0\n",
    "        }\n",
    "\n",
    "    def extract_premium_advanced(self, text):\n",
    "        if pd.isna(text):\n",
    "            return self._empty_premium_features()\n",
    "        text_lower = str(text).lower()\n",
    "\n",
    "        ultra_premium = ['imported', 'artisan', 'handcrafted', 'reserve', 'aged', 'vintage', 'estate']\n",
    "        premium = ['organic', 'premium', 'gourmet', 'specialty', 'select', 'finest', 'quality']\n",
    "        quality = ['natural', 'pure', 'authentic', 'traditional', 'fresh', 'real']\n",
    "        dietary = ['gluten-free', 'gluten free', 'non-gmo', 'non gmo', 'kosher',\n",
    "                  'halal', 'vegan', 'vegetarian', 'dairy-free', 'sugar-free']\n",
    "\n",
    "        ultra_premium_count = sum(1 for w in ultra_premium if w in text_lower)\n",
    "        premium_count = sum(1 for w in premium if w in text_lower)\n",
    "        quality_count = sum(1 for w in quality if w in text_lower)\n",
    "        dietary_count = sum(1 for w in dietary if w in text_lower)\n",
    "\n",
    "        weighted_premium = (ultra_premium_count * 2.5 + premium_count * 1.5 +\n",
    "                          quality_count * 0.8 + dietary_count * 1.2)\n",
    "\n",
    "        return {\n",
    "            'ultra_premium_count': ultra_premium_count,\n",
    "            'premium_count': premium_count,\n",
    "            'quality_count': quality_count,\n",
    "            'dietary_count': dietary_count,\n",
    "            'weighted_premium_score': weighted_premium,\n",
    "            'is_premium': 1 if (ultra_premium_count > 0 or premium_count > 0) else 0,\n",
    "        }\n",
    "\n",
    "    def _empty_premium_features(self):\n",
    "        return {k: 0 for k in ['ultra_premium_count', 'premium_count', 'quality_count',\n",
    "                               'dietary_count', 'weighted_premium_score', 'is_premium']}\n",
    "\n",
    "    def extract_category_v2(self, text):\n",
    "        if pd.isna(text):\n",
    "            return {'category': 'other', 'category_price_tier': 1}\n",
    "        text_lower = str(text).lower()\n",
    "\n",
    "        categories = {\n",
    "            'wine': 4, 'cheese': 3, 'olive_oil': 3, 'chocolate': 3,\n",
    "            'sauce': 2, 'dressing': 2, 'condiment': 2, 'cookie': 2,\n",
    "            'soup': 2, 'seasoning': 2, 'snack': 2, 'beverage': 2,\n",
    "            'powder': 1, 'water': 1, 'basic': 1\n",
    "        }\n",
    "\n",
    "        keywords = {\n",
    "            'wine': ['wine', 'vino', 'merlot', 'cabernet', 'chardonnay'],\n",
    "            'cheese': ['cheese', 'cheddar', 'mozzarella', 'parmesan', 'brie'],\n",
    "            'olive_oil': ['olive oil', 'extra virgin'],\n",
    "            'chocolate': ['chocolate', 'cocoa'],\n",
    "            'sauce': ['sauce', 'salsa', 'marinara'],\n",
    "            'dressing': ['dressing', 'vinaigrette'],\n",
    "            'condiment': ['ketchup', 'mustard', 'mayo', 'relish'],\n",
    "            'soup': ['soup', 'broth', 'stock'],\n",
    "            'seasoning': ['seasoning', 'spice', 'herb'],\n",
    "            'snack': ['chips', 'popcorn', 'crackers', 'pretzels'],\n",
    "            'beverage': ['juice', 'soda', 'drink', 'tea', 'coffee'],\n",
    "            'cookie': ['cookie', 'biscuit', 'wafer'],\n",
    "            'powder': ['powder', 'mix'],\n",
    "            'water': ['water'],\n",
    "            'basic': ['salt', 'pepper', 'sugar']\n",
    "        }\n",
    "\n",
    "        detected_category = 'other'\n",
    "        price_tier = 1\n",
    "        for cat, kws in keywords.items():\n",
    "            if any(kw in text_lower for kw in kws):\n",
    "                detected_category = cat\n",
    "                price_tier = categories.get(cat, 1)\n",
    "                break\n",
    "\n",
    "        return {'category': detected_category, 'category_price_tier': price_tier}\n",
    "\n",
    "    def extract_text_features_advanced(self, text):\n",
    "        if pd.isna(text):\n",
    "            return self._empty_text_features()\n",
    "        text = str(text)\n",
    "\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        unique_words = set(words)\n",
    "        bullets = re.findall(r'Bullet Point \\d+:', text)\n",
    "\n",
    "        return {\n",
    "            'char_count': len(text),\n",
    "            'word_count': len(words),\n",
    "            'unique_word_count': len(unique_words),\n",
    "            'bullet_count': len(bullets),\n",
    "            'log_char_count': np.log1p(len(text)),\n",
    "            'log_word_count': np.log1p(len(words)),\n",
    "            'word_diversity': len(unique_words) / (len(words) + 1),\n",
    "        }\n",
    "\n",
    "    def _empty_text_features(self):\n",
    "        return {k: 0 for k in ['char_count', 'word_count', 'unique_word_count',\n",
    "                               'bullet_count', 'log_char_count', 'log_word_count',\n",
    "                               'word_diversity']}\n",
    "\n",
    "    def compute_target_encoding(self, df, target_col='price', is_training=True):\n",
    "        brands = df['catalog_content'].apply(\n",
    "            lambda x: re.search(r'Item Name:\\s*([^,\\s]+)', str(x)).group(1).lower()\n",
    "            if pd.notna(x) and re.search(r'Item Name:\\s*([^,\\s]+)', str(x)) else 'unknown'\n",
    "        )\n",
    "\n",
    "        categories = df['catalog_content'].apply(self.extract_category_v2).apply(lambda x: x['category'])\n",
    "\n",
    "        if is_training:\n",
    "            global_mean = df[target_col].mean()\n",
    "\n",
    "            brand_stats = brands.to_frame('brand').join(df[[target_col]])\n",
    "            self.brand_stats = brand_stats.groupby('brand')[target_col].agg(['mean', 'count']).to_dict('index')\n",
    "            self.brand_mean = global_mean\n",
    "\n",
    "            cat_stats = categories.to_frame('category').join(df[[target_col]])\n",
    "            self.category_stats = cat_stats.groupby('category')[target_col].agg(['mean', 'count']).to_dict('index')\n",
    "            self.cat_mean = global_mean\n",
    "\n",
    "        m = 10\n",
    "        brand_encoded = brands.map(lambda x: self._smooth_mean(\n",
    "            self.brand_stats.get(x, {'mean': self.brand_mean, 'count': 0}),\n",
    "            self.brand_mean, m\n",
    "        ))\n",
    "\n",
    "        cat_encoded = categories.map(lambda x: self._smooth_mean(\n",
    "            self.category_stats.get(x, {'mean': self.cat_mean, 'count': 0}),\n",
    "            self.cat_mean, m\n",
    "        ))\n",
    "\n",
    "        return brand_encoded.values, cat_encoded.values\n",
    "\n",
    "    def _smooth_mean(self, stats, global_mean, m):\n",
    "        count = stats['count']\n",
    "        mean = stats['mean']\n",
    "        return (count * mean + m * global_mean) / (count + m)\n",
    "\n",
    "    def process_catalog_content(self, df, is_training=True, prices=None, image_features_df=None):\n",
    "        print(\"=\"*70)\n",
    "        print(\" \" * 20 + \"FEATURE EXTRACTION\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        features_dict = {}\n",
    "\n",
    "        item_names = df['catalog_content'].apply(\n",
    "            lambda x: re.search(r'Item Name:\\s*([^\\n]+)', str(x)).group(1)\n",
    "            if pd.notna(x) and re.search(r'Item Name:\\s*([^\\n]+)', str(x)) else ''\n",
    "        )\n",
    "        bullet_text = df['catalog_content'].apply(\n",
    "            lambda x: ' '.join(re.findall(r'Bullet Point \\d+:\\s*([^\\n]+)', str(x)))\n",
    "            if pd.notna(x) else ''\n",
    "        )\n",
    "\n",
    "        for key, func in [\n",
    "            ('size', self.extract_size_features_advanced),\n",
    "            ('premium', self.extract_premium_advanced),\n",
    "            ('category', self.extract_category_v2),\n",
    "            ('text', self.extract_text_features_advanced)\n",
    "        ]:\n",
    "            features = df['catalog_content'].apply(func)\n",
    "            for k in features.iloc[0].keys():\n",
    "                features_dict[k] = [f[k] for f in features]\n",
    "\n",
    "        numerical_df = pd.DataFrame(features_dict)\n",
    "\n",
    "        if prices is not None:\n",
    "            temp_df = df.copy()\n",
    "            temp_df['price'] = prices\n",
    "            brand_encoded, cat_encoded = self.compute_target_encoding(temp_df, is_training=is_training)\n",
    "        else:\n",
    "            if is_training:\n",
    "                raise ValueError(\"Prices required for training\")\n",
    "            brand_encoded, cat_encoded = self.compute_target_encoding(df, is_training=False)\n",
    "\n",
    "        numerical_df['brand_target_enc'] = brand_encoded\n",
    "        numerical_df['category_target_enc'] = cat_encoded\n",
    "\n",
    "        # TF-IDF features\n",
    "        if is_training:\n",
    "            tfidf_name = self.tfidf_name.fit_transform(item_names)\n",
    "            tfidf_bullets = self.tfidf_bullets.fit_transform(bullet_text)\n",
    "            char_features = self.char_vectorizer.fit_transform(item_names)\n",
    "        else:\n",
    "            tfidf_name = self.tfidf_name.transform(item_names)\n",
    "            tfidf_bullets = self.tfidf_bullets.transform(bullet_text)\n",
    "            char_features = self.char_vectorizer.transform(item_names)\n",
    "\n",
    "        tfidf_name_svd = self.svd_name.fit_transform(tfidf_name) if is_training else self.svd_name.transform(tfidf_name)\n",
    "        tfidf_bullets_svd = self.svd_bullets.fit_transform(tfidf_bullets) if is_training else self.svd_bullets.transform(tfidf_bullets)\n",
    "        char_svd = self.svd_char.fit_transform(char_features) if is_training else self.svd_char.transform(char_features)\n",
    "\n",
    "        tfidf_name_df = pd.DataFrame(tfidf_name_svd, columns=[f'name_tfidf_{i}' for i in range(tfidf_name_svd.shape[1])])\n",
    "        tfidf_bullets_df = pd.DataFrame(tfidf_bullets_svd, columns=[f'bullet_tfidf_{i}' for i in range(tfidf_bullets_svd.shape[1])])\n",
    "        char_df = pd.DataFrame(char_svd, columns=[f'char_ngram_{i}' for i in range(char_svd.shape[1])])\n",
    "\n",
    "        if is_training:\n",
    "            numerical_df['category_encoded'] = self.le_category.fit_transform(numerical_df['category'])\n",
    "        else:\n",
    "            test_categories = numerical_df['category']\n",
    "            known_categories = list(self.le_category.classes_)\n",
    "            numerical_df['category_encoded'] = test_categories.apply(\n",
    "                lambda x: self.le_category.transform([x])[0] if x in known_categories else -1\n",
    "            )\n",
    "\n",
    "        numerical_df = numerical_df.drop(['category'], axis=1)\n",
    "\n",
    "        # Interaction features\n",
    "        numerical_df['size_premium_int'] = numerical_df['size_oz'] * numerical_df['weighted_premium_score']\n",
    "        numerical_df['size_tier_int'] = numerical_df['size_oz'] * numerical_df['category_price_tier']\n",
    "        numerical_df['brand_cat_int'] = numerical_df['brand_target_enc'] * numerical_df['category_target_enc']\n",
    "        numerical_df['pack_premium_int'] = numerical_df['pack_count'] * numerical_df['weighted_premium_score']\n",
    "        numerical_df['brand_size_int'] = numerical_df['brand_target_enc'] * numerical_df['log_size']\n",
    "\n",
    "        image_features_list = []\n",
    "        if image_features_df is not None:\n",
    "            print(f\"\\nâœ“ Loading pre-extracted image features...\")\n",
    "            print(f\"  Image features shape: {image_features_df.shape}\")\n",
    "\n",
    "            img_feat = image_features_df.copy()\n",
    "            if 'sample_id' in img_feat.columns:\n",
    "                img_feat = img_feat.drop('sample_id', axis=1)\n",
    "\n",
    "            n_components = 65  # Increased from 50\n",
    "\n",
    "            if is_training:\n",
    "                print(f\"  Applying PCA: {img_feat.shape[1]} â†’ {n_components} components\")\n",
    "                self.image_pca = PCA(n_components=n_components, random_state=42)\n",
    "                img_reduced = self.image_pca.fit_transform(img_feat)\n",
    "                explained_var = self.image_pca.explained_variance_ratio_.sum()\n",
    "                print(f\"  Explained variance: {explained_var:.2%}\")\n",
    "            else:\n",
    "                if self.image_pca is None:\n",
    "                    raise RuntimeError(\"Image PCA not fitted. Train model first.\")\n",
    "                img_reduced = self.image_pca.transform(img_feat)\n",
    "\n",
    "            img_df = pd.DataFrame(img_reduced, columns=[f'img_pca_{i}' for i in range(n_components)])\n",
    "\n",
    "            # Enhanced image-text interactions\n",
    "            numerical_df['img_size_int'] = img_reduced[:, 0] * numerical_df['log_size']\n",
    "            numerical_df['img_premium_int'] = img_reduced[:, 1] * numerical_df['weighted_premium_score']\n",
    "            numerical_df['img_tier_int'] = img_reduced[:, 2] * numerical_df['category_price_tier']\n",
    "            numerical_df['img_brand_int'] = img_reduced[:, 3] * numerical_df['brand_target_enc']\n",
    "            numerical_df['img_pack_int'] = img_reduced[:, 4] * numerical_df['log_pack']\n",
    "            numerical_df['img_quality_int'] = img_reduced[:, 5] * numerical_df['quality_count']\n",
    "\n",
    "            image_features_list = [img_df]\n",
    "            print(f\"âœ“ Added {n_components} PCA image features + 6 interaction features\")\n",
    "\n",
    "        final_features = pd.concat([\n",
    "            numerical_df.reset_index(drop=True),\n",
    "            tfidf_name_df.reset_index(drop=True),\n",
    "            tfidf_bullets_df.reset_index(drop=True),\n",
    "            char_df.reset_index(drop=True)\n",
    "        ] + image_features_list, axis=1)\n",
    "\n",
    "        final_features = final_features.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        print(f\"âœ“ Total features: {final_features.shape[1]}\\n\")\n",
    "        return final_features\n",
    "\n",
    "\n",
    "# ==================== ENHANCED NEURAL NETWORK ====================\n",
    "\n",
    "class EnhancedPricePredictor(nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.50):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_bn = nn.BatchNorm1d(input_dim)\n",
    "        self.input_dropout = nn.Dropout(0.20)\n",
    "\n",
    "        # Deeper architecture with residual connections\n",
    "        self.fc1 = nn.Linear(input_dim, 384)\n",
    "        self.bn1 = nn.BatchNorm1d(384)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc2 = nn.Linear(384, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.dropout3 = nn.Dropout(dropout * 0.75)\n",
    "\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.dropout4 = nn.Dropout(dropout * 0.5)\n",
    "\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.bn5 = nn.BatchNorm1d(32)\n",
    "        self.dropout5 = nn.Dropout(dropout * 0.3)\n",
    "\n",
    "        self.output = nn.Linear(32, 1)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_bn(x)\n",
    "        x = self.input_dropout(x)\n",
    "\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = F.relu(self.bn5(self.fc5(x)))\n",
    "        x = self.dropout5(x)\n",
    "\n",
    "        output = self.output(x)\n",
    "        return output.squeeze()\n",
    "\n",
    "\n",
    "class ImprovedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.huber = nn.SmoothL1Loss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred_orig = torch.expm1(pred)\n",
    "        target_orig = torch.expm1(target)\n",
    "\n",
    "        mse_loss = F.mse_loss(pred, target)\n",
    "        huber_loss = self.huber(pred, target)\n",
    "\n",
    "        epsilon = 0.1\n",
    "        smape_loss = 200 * torch.mean(\n",
    "            torch.abs(pred_orig - target_orig) / (torch.abs(pred_orig) + torch.abs(target_orig) + epsilon)\n",
    "        )\n",
    "\n",
    "        weights = torch.where(target_orig < 10, 1.8,\n",
    "                   torch.where(target_orig < 20, 1.3,\n",
    "                   torch.where(target_orig < 50, 1.1, 1.0)))\n",
    "        weighted_mse = torch.mean(weights * (pred - target) ** 2)\n",
    "\n",
    "        total_loss = 0.20 * weighted_mse + 0.20 * huber_loss + 0.60 * smape_loss\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    y_true_np = y_true.cpu().numpy() if torch.is_tensor(y_true) else y_true\n",
    "    y_pred_np = y_pred.cpu().numpy() if torch.is_tensor(y_pred) else y_pred\n",
    "\n",
    "    smape = 100 * np.mean(np.abs(y_pred_np - y_true_np) / ((np.abs(y_pred_np) + np.abs(y_true_np))/2) + 1e-8)\n",
    "    mse = np.mean((y_pred_np - y_true_np) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_pred_np - y_true_np))\n",
    "    mape = 100 * np.mean(np.abs((y_true_np - y_pred_np) / (y_true_np + 1e-8)))\n",
    "    ss_res = np.sum((y_true_np - y_pred_np) ** 2)\n",
    "    ss_tot = np.sum((y_true_np - np.mean(y_true_np)) ** 2)\n",
    "    r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "\n",
    "    return {'SMAPE': smape, 'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'R2': r2}\n",
    "\n",
    "\n",
    "# ==================== TRAINING WITH K-FOLD ====================\n",
    "\n",
    "def train_with_kfold(df, train_image_df, n_folds=5):\n",
    "    print(\"=\"*70)\n",
    "    print(\" \" * 15 + \"K-FOLD CROSS-VALIDATION TRAINING\")\n",
    "    print(\" \" * 22 + \"(WITH IMAGE FEATURES)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    extractor = OptimizedFeatureExtractor()\n",
    "    y = df['price'].values\n",
    "    y_log = np.log1p(y)\n",
    "\n",
    "    X = extractor.process_catalog_content(df, is_training=True, prices=y, image_features_df=train_image_df)\n",
    "\n",
    "    print(f\"Dataset: {len(df)} samples, {X.shape[1]} features\")\n",
    "    print(f\"Price range: ${y.min():.2f} - ${y.max():.2f}\\n\")\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"âœ“ Using device: {device}\\n\")\n",
    "\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_models = []\n",
    "    fold_smapes = []\n",
    "    fold_predictions = np.zeros(len(df))\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FOLD {fold + 1}/{n_folds}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        X_val_fold = X.iloc[val_idx]\n",
    "        y_train_fold = y_log[train_idx]\n",
    "        y_val_fold = y_log[val_idx]\n",
    "        y_val_orig = y[val_idx]\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_fold)\n",
    "        X_val_scaled = scaler.transform(X_val_fold)\n",
    "\n",
    "        X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "        X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "        y_train_tensor = torch.FloatTensor(y_train_fold).to(device)\n",
    "\n",
    "        batch_size = 384\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        model = EnhancedPricePredictor(\n",
    "            input_dim=X_train_scaled.shape[1],\n",
    "            dropout=0.50\n",
    "        ).to(device)\n",
    "\n",
    "        criterion = ImprovedLoss()\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=0.0008,\n",
    "            betas=(0.9, 0.999),\n",
    "            weight_decay=0.0075\n",
    "        )\n",
    "\n",
    "        num_epochs = 200\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=40, T_mult=2, eta_min=3e-7\n",
    "        )\n",
    "\n",
    "        best_val_smape = float('inf')\n",
    "        best_model_state = None\n",
    "        patience = 30\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.2)\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * batch_X.size(0)\n",
    "\n",
    "            train_loss /= len(train_dataset)\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred_log = model(X_val_tensor).cpu().numpy()\n",
    "                val_pred = np.expm1(val_pred_log)\n",
    "                val_pred = np.clip(val_pred, 0.01, 10000)\n",
    "\n",
    "                val_smape = 100 * np.mean(\n",
    "                    np.abs(val_pred - y_val_orig) / (((np.abs(val_pred) + np.abs(y_val_orig))/2 + 1e-8))\n",
    "                )\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            if val_smape < best_val_smape:\n",
    "                best_val_smape = val_smape\n",
    "                best_model_state = model.state_dict()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if (epoch + 1) % 25 == 0 or epoch < 3:\n",
    "                print(f\"Epoch {epoch+1:3d}/{num_epochs} | Loss: {train_loss:.4f} | Val SMAPE: {val_smape:.2f}\")\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Load best model\n",
    "        model.load_state_dict(best_model_state)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_pred_log = model(X_val_tensor).cpu().numpy()\n",
    "            val_pred = np.expm1(val_pred_log)\n",
    "            val_pred = np.clip(val_pred, 0.01, 10000)\n",
    "\n",
    "        fold_predictions[val_idx] = val_pred\n",
    "\n",
    "        print(f\"\\nFold {fold + 1} Best SMAPE: {best_val_smape:.2f}\")\n",
    "        fold_smapes.append(best_val_smape)\n",
    "        fold_models.append({\n",
    "            'model': model,\n",
    "            'scaler': scaler,\n",
    "            'best_smape': best_val_smape\n",
    "        })\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\" \" * 20 + \"CROSS-VALIDATION RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    cv_metrics = calculate_metrics(y, fold_predictions)\n",
    "\n",
    "    print(f\"\\nOverall CV Metrics:\")\n",
    "    for metric, value in cv_metrics.items():\n",
    "        print(f\"  {metric:<10} {value:>10.4f}\")\n",
    "\n",
    "    print(f\"\\nFold SMAPE scores:\")\n",
    "    for i, smape in enumerate(fold_smapes):\n",
    "        print(f\"  Fold {i+1}: {smape:.2f}\")\n",
    "    print(f\"  Mean: {np.mean(fold_smapes):.2f} Â± {np.std(fold_smapes):.2f}\")\n",
    "\n",
    "    # Error analysis by price range\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\" \" * 23 + \"SMAPE BY PRICE RANGE\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    price_ranges = [\n",
    "        (0, 10, 'Low ($0-$10)'),\n",
    "        (10, 20, 'Medium ($10-$20)'),\n",
    "        (20, 50, 'High ($20-$50)'),\n",
    "        (50, float('inf'), 'Very High (>$50)')\n",
    "    ]\n",
    "\n",
    "    for low, high, label in price_ranges:\n",
    "        mask = (y >= low) & (y < high)\n",
    "        if mask.sum() > 0:\n",
    "            range_smape = 200 * np.mean(\n",
    "                np.abs(fold_predictions[mask] - y[mask]) /\n",
    "                (np.abs(fold_predictions[mask]) + np.abs(y[mask]) + 1e-8)\n",
    "            )\n",
    "            range_mae = np.mean(np.abs(fold_predictions[mask] - y[mask]))\n",
    "            print(f\"{label:<25} SMAPE: {range_smape:>6.2f}  MAE: ${range_mae:>6.2f}  (n={mask.sum()})\")\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\" \" * 20 + \"ðŸŽ¯ PERFORMANCE SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    print(f\"\\nâœ“ CV SMAPE:  {cv_metrics['SMAPE']:.2f}\")\n",
    "    print(f\"âœ“ CV RMSE:   ${cv_metrics['RMSE']:.2f}\")\n",
    "    print(f\"âœ“ CV MAE:    ${cv_metrics['MAE']:.2f}\")\n",
    "    print(f\"âœ“ RÂ² Score:  {cv_metrics['R2']:.4f}\")\n",
    "\n",
    "    if cv_metrics['SMAPE'] < 45:\n",
    "        print(f\"\\nðŸŽ‰ TARGET ACHIEVED! SMAPE < 45 âœ“\")\n",
    "    else:\n",
    "        print(f\"\\nâš  Gap to target (45): {cv_metrics['SMAPE'] - 45:.2f} points\")\n",
    "\n",
    "    print(f\"\\nImprovement from baseline (54.3): {54.3 - cv_metrics['SMAPE']:.2f} points\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    return {\n",
    "        'fold_models': fold_models,\n",
    "        'extractor': extractor,\n",
    "        'device': device,\n",
    "        'cv_metrics': cv_metrics,\n",
    "        'fold_smapes': fold_smapes\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_test_data(test_df, test_image_df, trained_results):\n",
    "    \"\"\"Make ensemble predictions on test data\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" \" * 20 + \"TEST DATA PREDICTION\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    extractor = trained_results['extractor']\n",
    "    fold_models = trained_results['fold_models']\n",
    "    device = trained_results['device']\n",
    "\n",
    "    print(f\"\\nProcessing {len(test_df)} test samples...\")\n",
    "\n",
    "    X_test = extractor.process_catalog_content(test_df, is_training=False, image_features_df=test_image_df)\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    for i, fold_data in enumerate(fold_models):\n",
    "        model = fold_data['model']\n",
    "        scaler = fold_data['scaler']\n",
    "\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_pred_log = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "        test_pred = np.expm1(test_pred_log)\n",
    "        test_pred = np.clip(test_pred, 0.01, 10000)\n",
    "\n",
    "        all_predictions.append(test_pred)\n",
    "        print(f\"  Fold {i+1} predictions: ${test_pred.mean():.2f} mean\")\n",
    "\n",
    "    # Weighted ensemble\n",
    "    weights = [1.0 / fold_data['best_smape'] for fold_data in fold_models]\n",
    "    weights = np.array(weights) / np.sum(weights)\n",
    "    weighted_predictions = np.average(all_predictions, axis=0, weights=weights)\n",
    "\n",
    "    print(f\"\\nâœ“ Predictions complete!\")\n",
    "    print(f\"\\nEnsemble (weighted by performance):\")\n",
    "    print(f\"  Price range: ${weighted_predictions.min():.2f} - ${weighted_predictions.max():.2f}\")\n",
    "    print(f\"  Mean price:  ${weighted_predictions.mean():.2f}\")\n",
    "    print(f\"  Median:      ${np.median(weighted_predictions):.2f}\")\n",
    "\n",
    "    return weighted_predictions\n",
    "\n",
    "\n",
    "def create_submission(test_df, predictions, output_file='submission.csv'):\n",
    "    \"\"\"Create submission file\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" \" * 20 + \"CREATING SUBMISSION\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        'sample_id': test_df['sample_id'],\n",
    "        'price': predictions\n",
    "    })\n",
    "\n",
    "    submission.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"\\nâœ“ File saved: {output_file}\")\n",
    "    print(f\"âœ“ Total predictions: {len(submission)}\")\n",
    "    print(f\"\\nFirst 10 rows:\")\n",
    "    print(submission.head(10).to_string(index=False))\n",
    "    print(f\"\\nStats:\")\n",
    "    print(f\"  Min:    ${predictions.min():.2f}\")\n",
    "    print(f\"  Max:    ${predictions.max():.2f}\")\n",
    "    print(f\"  Mean:   ${predictions.mean():.2f}\")\n",
    "    print(f\"  Median: ${np.median(predictions):.2f}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    return submission\n",
    "\n",
    "\n",
    "def full_pipeline(train_csv='train.csv', test_csv='test.csv',\n",
    "                  train_image_csv='image_reduced_256.csv',\n",
    "                  test_image_csv='test_image_features_256.csv',\n",
    "                  output_csv='submission.csv', n_folds=5):\n",
    "    \"\"\"Complete optimized pipeline\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" \" * 15 + \"ðŸš€ OPTIMIZED PIPELINE START\")\n",
    "    print(\" \" * 18 + \"(TARGET: SMAPE ~45)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Step 1: Load training data\n",
    "    print(\"\\n[1/5] Loading training data...\")\n",
    "    train_df = pd.read_csv(train_csv)\n",
    "    train_image_df = pd.read_csv(train_image_csv)\n",
    "    print(f\"âœ“ Loaded {len(train_df)} training samples\")\n",
    "    print(f\"âœ“ Loaded {train_image_df.shape} image features\")\n",
    "\n",
    "    if 'sample_id' in train_image_df.columns:\n",
    "        if not train_df['sample_id'].equals(train_image_df['sample_id']):\n",
    "            print(\"âš  Aligning image features by sample_id...\")\n",
    "            train_image_df = train_image_df.set_index('sample_id').loc[train_df['sample_id']].reset_index(drop=True)\n",
    "\n",
    "    # Step 2: Train model\n",
    "    print(f\"\\n[2/5] Training with {n_folds}-Fold Cross-Validation...\")\n",
    "    trained_results = train_with_kfold(train_df, train_image_df, n_folds=n_folds)\n",
    "\n",
    "    # Step 3: Load test data\n",
    "    print(\"\\n[3/5] Loading test data...\")\n",
    "    test_df = pd.read_csv(test_csv)\n",
    "    test_image_df = pd.read_csv(test_image_csv)\n",
    "    print(f\"âœ“ Loaded {len(test_df)} test samples\")\n",
    "    print(f\"âœ“ Loaded {test_image_df.shape} test image features\")\n",
    "\n",
    "    if 'sample_id' in test_image_df.columns:\n",
    "        if not test_df['sample_id'].equals(test_image_df['sample_id']):\n",
    "            print(\"âš  Aligning test image features by sample_id...\")\n",
    "            test_image_df = test_image_df.set_index('sample_id').loc[test_df['sample_id']].reset_index(drop=True)\n",
    "\n",
    "    # Step 4: Make predictions\n",
    "    print(\"\\n[4/5] Making predictions...\")\n",
    "    predictions = predict_test_data(test_df, test_image_df, trained_results)\n",
    "\n",
    "    # Step 5: Create submission\n",
    "    print(\"\\n[5/5] Creating submission file...\")\n",
    "    submission = create_submission(test_df, predictions, output_csv)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" \" * 15 + \"âœ… PIPELINE COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nðŸ“Š Cross-Validation SMAPE: {trained_results['cv_metrics']['SMAPE']:.2f}\")\n",
    "    print(f\"ðŸ“„ Submission file: {output_csv}\")\n",
    "\n",
    "    if trained_results['cv_metrics']['SMAPE'] < 45:\n",
    "        print(f\"\\nðŸŽ¯ SUCCESS! Target SMAPE < 45 achieved!\")\n",
    "    else:\n",
    "        print(f\"\\nðŸ“ˆ Current SMAPE: {trained_results['cv_metrics']['SMAPE']:.2f}\")\n",
    "        print(f\"   Gap to target: {trained_results['cv_metrics']['SMAPE'] - 45:.2f} points\")\n",
    "\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    return {\n",
    "        'trained_results': trained_results,\n",
    "        'predictions': predictions,\n",
    "        'submission': submission\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = full_pipeline(\n",
    "        train_csv='/content/train.csv',\n",
    "        test_csv='/content/test.csv',\n",
    "        train_image_csv='image_reduced_256.csv',\n",
    "        test_image_csv='test_image_features_256.csv',\n",
    "        output_csv='submission_new.csv',\n",
    "        n_folds=5\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL RESULTS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nCross-Validation Metrics:\")\n",
    "    for metric, value in results['trained_results']['cv_metrics'].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    print(f\"\\nPredictions saved to: submission_new.csv\")\n",
    "    print(f\"Total test predictions: {len(results['predictions'])}\")\n",
    "    print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
